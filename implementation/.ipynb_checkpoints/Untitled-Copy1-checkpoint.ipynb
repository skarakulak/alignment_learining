{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "import multiprocessing as mp\n",
    "from multiprocessing.pool import ThreadPool\n",
    "from src import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense(x, scope, num_h, n_x, haveBias = True):\n",
    "    \"\"\"\n",
    "    Standard affine layer\n",
    "    \n",
    "    scope = name tf variable scope\n",
    "    num_h = number of hidden units\n",
    "    num_x = number of input units\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(scope):\n",
    "        w = tf.get_variable('w', [n_x, num_h], initializer=tf.random_normal_initializer(stddev=0.04))\n",
    "        if(haveBias):\n",
    "            b = tf.get_variable('b', [num_h], initializer=tf.constant_initializer(0))\n",
    "            return(tf.matmul(x, w)+b)\n",
    "        else:\n",
    "            return(tf.matmul(x, w))\n",
    "\n",
    "def lrelu(x, alpha):\n",
    "    return(tf.nn.relu(x) - alpha * tf.nn.relu(-x))\n",
    "    \n",
    "def conv(x, scope, filter_h,filter_w, n_kernel, stride_h=1,stride_w=1, padding='SAME'):\n",
    "    \"\"\"\n",
    "    Convolutional layer\n",
    "    \n",
    "    scope        = name tf variable scope\n",
    "    filter_h     = height of the receptive field\n",
    "    filter_w     = width of the receptive field\n",
    "    n_kernel     = # of kernels\n",
    "    stride_h     = stride height\n",
    "    stride_w     = stride width\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(scope, reuse=False):\n",
    "        n_x = x.get_shape().as_list()[-1]\n",
    "        w = tf.get_variable('w',\n",
    "                            [filter_h, filter_w, n_x, n_kernel],\n",
    "                            initializer=tf.random_normal_initializer(stddev=0.04))\n",
    "        b = tf.get_variable('b', [n_kernel], initializer=tf.constant_initializer(0))\n",
    "        return tf.nn.convolution(x, w, padding=padding, strides=[stride_h, stride_w])+b    \n",
    "\n",
    "\n",
    "def bnorm(X,isTraining,scope='batch_norm',axis=-1):\n",
    "    \"\"\"\n",
    "    Batch normalization layer\n",
    "    \n",
    "    X          = input\n",
    "    isTraining = True during training, False otherwise.\n",
    "    axis       = axis for normalization\n",
    "    scope      = name tf variable scope\n",
    "    \n",
    "    \"\"\"\n",
    "    return(tf.layers.batch_normalization(\n",
    "        inputs=X,\n",
    "        axis=axis, \n",
    "        training = isTraining,\n",
    "        name=scope\n",
    "    ))\n",
    "\n",
    "\n",
    "class Attention:\n",
    "    \"\"\"\n",
    "    Attention class\n",
    "    Reference:\n",
    "    https://github.com/tensorflow/models/blob/master/official/transformer/model/attention_layer.py\n",
    "    https://github.com/DongjunLee/transformer-tensorflow/blob/master/transformer/attention.py\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        nObs,\n",
    "        hiddenSize,\n",
    "        numHeads=1,\n",
    "        dropout=0.2,\n",
    "        isTraining=True\n",
    "        ):\n",
    "        assert hiddenSize % numHeads == 0\n",
    "        self.nObs=nObs\n",
    "        self.hiddenSize = hiddenSize\n",
    "        self.numHeads = numHeads\n",
    "        self.dropout = dropout\n",
    "        self.isTraining=isTraining\n",
    "        self.dimPerHead = hiddenSize//numHeads\n",
    "\n",
    "    def multi_head(self, q, k, v):\n",
    "        q, k, v = self._linear_projection(q, k, v)\n",
    "        qs, ks, vs = self._split_heads(q, k, v)\n",
    "        outputs = self._scaled_dot_product(qs, ks, vs)\n",
    "        output = self._concat_heads(outputs)\n",
    "        output = tf.layers.dense(output, self.hiddenSize)\n",
    "        return(output)\n",
    "        #return(tf.nn.dropout(output, 1.0 - self.dropout))\n",
    "\n",
    "    def _linear_projection(self, q, k, v):\n",
    "        q = tf.layers.dense(q, self.hiddenSize, use_bias=False)#,activation=tf.nn.relu)\n",
    "        k = tf.layers.dense(k, self.hiddenSize, use_bias=False)#,activation=tf.nn.relu)\n",
    "        v = tf.layers.dense(v, self.hiddenSize, use_bias=False)#,activation=tf.nn.relu)\n",
    "        return(q,k,v)\n",
    "\n",
    "    def _split_heads(self, q, k, v):\n",
    "        def splitAndTranspose(x, numHeads, hiddenSize):\n",
    "            t_shape = x.get_shape().as_list()\n",
    "            x = tf.reshape(x, [-1] + t_shape[1:-1] + [numHeads, hiddenSize // numHeads])\n",
    "            return( tf.transpose(x, [0, 2, 1, 3]) )# [batchSize, numHeads, numOfObs, hiddenSize//numHeads]\n",
    "\n",
    "        qs = splitAndTranspose(q, self.numHeads, self.hiddenSize)\n",
    "        ks = splitAndTranspose(k, self.numHeads, self.hiddenSize)\n",
    "        vs = splitAndTranspose(v, self.numHeads, self.hiddenSize)\n",
    "        return(qs,ks,vs)\n",
    "\n",
    "    def _scaled_dot_product(self, qs, ks, vs):\n",
    "        o = tf.matmul(qs, ks, transpose_b=True)\n",
    "        o /= self.dimPerHead**0.5\n",
    "        o = tf.nn.softmax(o)\n",
    "        return tf.matmul(o, vs)\n",
    "\n",
    "    def _concat_heads(self, outputs):\n",
    "        def transpose_then_concat_last_two_dimenstion(tensor):\n",
    "            tensor = tf.transpose(tensor, [0, 2, 1, 3]) # [batch_size, numOfObs, numHeads, dim]\n",
    "            t_shape = tensor.get_shape().as_list()\n",
    "            numHeads, dim = t_shape[-2:]\n",
    "            return(tf.reshape(tensor, [-1] + t_shape[1:-2] + [numHeads * dim]) )\n",
    "\n",
    "        return(transpose_then_concat_last_two_dimenstion(outputs))\n",
    "\n",
    "\n",
    "def convObservations(X,batch_size,n_images,isTraining=False, reuse=False):\n",
    "    \"\"\"\n",
    "    Makes convolutions over noisy signals with cyclic shifts.\n",
    "    X.size = (batch_size,nObservations, signalDim,1 ). Returns the average \n",
    "    value of the encodings of the observations\n",
    "    \"\"\"\n",
    "\n",
    "    with tf.variable_scope('convObservations', reuse=reuse):\n",
    "        h = lrelu(conv(X, 'conv0',1,5,32,1,1), 0.1)  # 5x64\n",
    "        h = bnorm(h,isTraining,'bnorm_1d_0')\n",
    "        h = lrelu(conv(h, 'conv1',1,3,64,1,1), 0.1)  # 5x64\n",
    "        h = bnorm(h,isTraining,'bnorm_1d_1')\n",
    "        h = lrelu(conv(h, 'conv2',1,3,128,1,2), 0.1)  # 3x128\n",
    "        h = bnorm(h,isTraining,'bnorm_1d_2')\n",
    "        h = lrelu(conv(h, 'conv3',1,3,256,1,2), 0.1)  # 2x128\n",
    "        h = bnorm(h,isTraining,'bnorm_1d_3')\n",
    "        h = tf.reshape(h, [-1, n_images, 512])\n",
    "\n",
    "        h = lrelu(tf.layers.dense(h, 512, name='fc_0'),0.1)\n",
    "        h = bnorm(h,isTraining,'bnorm_1d_fc0')\n",
    "        h = lrelu(tf.layers.dense(h, 512, name='fc_1'),0.1)\n",
    "\n",
    "        attn = Attention(\n",
    "            nObs=n_images,\n",
    "            hiddenSize=512,\n",
    "            numHeads=8,\n",
    "            dropout=0.1,\n",
    "            isTraining=isTraining\n",
    "            )\n",
    "\n",
    "        h = tf.add(attn.multi_head(h,h,h),h)\n",
    "\n",
    "        hff = lrelu(tf.layers.dense(h, 512, name='fc_2'),0.1)\n",
    "        hff = bnorm(hff,isTraining,'bnorm_1d_fc2')\n",
    "        hff = lrelu(tf.layers.dense(h, 512, name='fc_3'),0.1)\n",
    "        h = tf.add(hff,h)\n",
    "        \n",
    "        h = tf.reduce_mean(h, 1)\n",
    "        return(h)        \n",
    "\n",
    "\n",
    "    \n",
    "def decodeSignal(X, batch_size, enc_dim = 512, isTraining=False, reuse=False):\n",
    "    \"\"\"\n",
    "    Takes encoding produced by the observations as input and\n",
    "    generates a the underlying true signal\n",
    "    \"\"\"\n",
    "    with tf.variable_scope('decodeSignal', reuse=reuse):\n",
    "        h = lrelu(dense(X, 'hz0', num_h=256,n_x=enc_dim), 0.1)\n",
    "        h = tf.concat([X, h], 1)\n",
    "        h = bnorm(h,isTraining,'bnorm_hz0')\n",
    "        h = lrelu(dense(h, 'hz1', num_h=128,n_x=256+enc_dim), 0.1)\n",
    "        h = tf.concat([X, h], 1)\n",
    "        h = bnorm(h,isTraining,'bnorm_hz1')\n",
    "        h = lrelu(dense(h, 'hz2', num_h=64,n_x=128+enc_dim), 0.1)\n",
    "        h = tf.concat([X, h], 1)\n",
    "        h = bnorm(h,isTraining,'bnorm_hz2')\n",
    "        h = lrelu(dense(h, 'hz3', num_h=32,n_x=64+enc_dim), 0.1)\n",
    "        h = tf.concat([X, h], 1)\n",
    "        h = bnorm(h,isTraining,'bnorm_hz3')\n",
    "        h = lrelu(dense(h, 'hz4', num_h=16,n_x=32+enc_dim), 0.1)\n",
    "        h = tf.concat([X, h], 1)\n",
    "        h = bnorm(h,isTraining,'bnorm_hz4')\n",
    "        h = lrelu(dense(h, 'hz5', num_h=8,n_x=16+enc_dim), 0.1)\n",
    "        h = tf.concat([X, h], 1)\n",
    "        #h = bnorm(h,isTraining,'bnorm_hz5')\n",
    "        h = lrelu(dense(h, 'hz6', num_h=5,n_x=8+enc_dim), 0.1)\n",
    "        h = dense(h, 'z', num_h=5,n_x=5)\n",
    "        \n",
    "        return(h)    \n",
    "    \n",
    "\n",
    "class objGenNetwork(object):\n",
    "    \"\"\"\n",
    "    Implementation of the model\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 signalDim = 5,\n",
    "                 nObservationsPerSignal = 64,\n",
    "                 noise = 2,\n",
    "                 minibatchSize = 64,\n",
    "                 testSampleSize = 1000,\n",
    "                 lr = 0.001,\n",
    "                 training = True,\n",
    "                 skipStep = 1,\n",
    "                 nProcessesDataPrep=4,\n",
    "                 vers='NOT_SPECIFIED',\n",
    "                 evalAfterStep=0,\n",
    "                 evalNTimes=1\n",
    "                ):\n",
    "        self.signalDim = signalDim\n",
    "        self.nObservationsPerSignal = nObservationsPerSignal\n",
    "        self.noise = noise\n",
    "        self.minibatchSize = minibatchSize\n",
    "        self.testSampleSize = testSampleSize\n",
    "        self.lr = lr\n",
    "        self.isTraining = training\n",
    "        self.skipStep = skipStep\n",
    "        self.nProcessesDataPrep = nProcessesDataPrep\n",
    "        if (vers=='NOT_SPECIFIED'):\n",
    "            self.vers = str(signalDim)+'D'+'_sigma_'+str(noise) +'_obs_' + str(nObservationsPerSignal )\n",
    "        else:\n",
    "            self.vers = vers\n",
    "        self.logFile = 'log_'+ self.vers +'.txt'\n",
    "        self.evalAfterStep = evalAfterStep\n",
    "        self.evalNTimes = evalNTimes\n",
    "        \n",
    "        self.gstep = tf.Variable(0, \n",
    "                                 dtype=tf.int32, \n",
    "                                 trainable=False,\n",
    "                                 name='global_step')\n",
    "        self.train_x = None\n",
    "        self.train_y = None\n",
    "        self.test_x = None\n",
    "        self.test_y = None\n",
    "\n",
    "    def data_generator(self,trainingBatch=True):\n",
    "\n",
    "        \"\"\"\n",
    "        Generates batches of random signals and their noisy \n",
    "        observations with cyclic shifts. |Signal| = dim(Signal)\n",
    "        \"\"\"\n",
    "        minibatchSize = self.minibatchSize\n",
    "        testSampleSize = self.testSampleSize\n",
    "        signalDim = self.signalDim\n",
    "        noise = self.noise\n",
    "        nObservationsPerSignal=self.nObservationsPerSignal\n",
    "        \n",
    "        \n",
    "        batches = minibatchSize if trainingBatch else testSampleSize\n",
    "\n",
    "        \n",
    "        poolData = mp.Pool(processes= self.nProcessesDataPrep)\n",
    "        results = poolData.starmap(genSignal,[(\n",
    "            signalDim,\n",
    "            nObservationsPerSignal,\n",
    "            noise)] * batches )\n",
    "        poolData.close(); poolData.join()\n",
    "\n",
    "        batch_x = np.expand_dims(\n",
    "            np.array([k[1] for k in results],dtype='float32'),\n",
    "            axis=3\n",
    "        )\n",
    "        batch_y = np.array([k[0] for k in results],dtype='float32')\n",
    "        \n",
    "\n",
    "        if trainingBatch:\n",
    "            self.train_x_new = batch_x\n",
    "            self.train_y_new = batch_y\n",
    "        else:\n",
    "            self.test_x_new = batch_x\n",
    "            self.test_y_new = batch_y\n",
    "        \n",
    "        \n",
    "    def inference(self):\n",
    "        h = convObservations(self.x_ph, \n",
    "                             self.minibatchSize,\n",
    "                             self.nObservationsPerSignal,\n",
    "                             isTraining=self.isTraining\n",
    "                            )\n",
    "        self.preds = decodeSignal(h, \n",
    "                                  self.minibatchSize,\n",
    "                                  isTraining=self.isTraining\n",
    "                                 )\n",
    "\n",
    "        \n",
    "    def loss(self):\n",
    "        \"\"\"\n",
    "        Defines loss function\n",
    "        We use mean squared loss over the predicted and the true signal\n",
    "        under the best fitting cyclic shift.\n",
    "        \"\"\"\n",
    "        # \n",
    "        with tf.name_scope('loss'):\n",
    "            tiled_preds = tf.tile(tf.expand_dims(self.preds, 1),[1,self.signalDim,1])\n",
    "            entropy = tf.squared_difference(self.y_ph,tiled_preds)\n",
    "            entropy = tf.reduce_sum(entropy, axis = 2)\n",
    "            entropy = tf.reduce_min(entropy,axis = 1)\n",
    "            self.loss = tf.reduce_mean(entropy, name='loss')\n",
    "\n",
    "    def optimize(self):\n",
    "        \"\"\"\n",
    "        Optimization op\n",
    "        \"\"\"\n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(update_ops):\n",
    "#            self.opt = tf.train.AdamOptimizer(self.lr).minimize(self.loss, \n",
    "#                                                                   global_step=self.gstep)\n",
    "            \n",
    "            optimizer = tf.train.GradientDescentOptimizer(self.lr)\n",
    "            grads_and_vars = optimizer.compute_gradients(self.loss)\n",
    "       \t    with tf.name_scope('dropgrad'):\n",
    "                grads_and_vars = [(tf.nn.dropout(g,0.05), v) for g, v in grads_and_vars]\n",
    "            self.opt = optimizer.apply_gradients(grads_and_vars,global_step=self.gstep)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def additionalEvalMetrics(self):\n",
    "        \"\"\"\n",
    "        Takes the most accurate rotation of the true object\n",
    "        as reference and calculates the average accuracy of\n",
    "        the occupancy grid of the object\n",
    "        \"\"\"\n",
    "        with tf.name_scope('prediction_eval'):\n",
    "            tiledPreds = tf.tile(tf.expand_dims(self.preds, 1),\n",
    "                                 [1,self.signalDim,1])\n",
    "            MAE = tf.reduce_sum(tf.abs(tiledPreds - self.y_ph),\n",
    "                                axis = 2)\n",
    "            MAE = tf.reduce_min(MAE,axis=1)\n",
    "            self.MAE = tf.reduce_mean(MAE,name=\"MAE\")\n",
    "        \n",
    "        \n",
    "    def testEval(self, sess, writer, epoch, step,evalNTimes):\n",
    "        self.isTraining = False\n",
    "        l2_arr = np.zeros((evalNTimes,))\n",
    "        mae_arr = np.zeros((evalNTimes,))\n",
    "        for i in range(evalNTimes):\n",
    "            self.test_x = np.copy(self.test_x_new)\n",
    "            self.test_y = np.copy(self.test_y_new)\n",
    "            \n",
    "            pool = ThreadPool(processes=1)\n",
    "            async_result = pool.apply_async(self.data_generator,(False,))\n",
    "            mae_arr[i], l2_arr[i] = sess.run(\n",
    "                    [self.MAE,self.loss],\n",
    "                     feed_dict={self.x_ph: self.test_x,\n",
    "                                self.y_ph: self.test_y}\n",
    "                     )\n",
    "            pool.close()\n",
    "            pool.join()\n",
    "\n",
    "\n",
    "        if (self.logFile==False):\n",
    "            print('test MAE at step {0:.6}: {1:.6} '.format(step,mae_arr.mean()))\n",
    "            print('test loss at step {0:.6}: {1:.6} '.format(step,l2_arr.mean()))\n",
    "        else:\n",
    "            with open(self.logFile,'a') as lgfile:\n",
    "                lgfile.write('{0}\\t{1:.6}\\t{2:.6}\\n'.format(step,l2_arr.mean(),mae_arr.mean()))\n",
    "            \n",
    "\n",
    "    def summary(self):\n",
    "        \"\"\"\n",
    "        Summary for TensorBoard\n",
    "        \"\"\"\n",
    "        with tf.name_scope('summaries'):\n",
    "            tf.summary.scalar('loss', self.loss)\n",
    "            tf.summary.scalar('MAE', self.MAE)\n",
    "            tf.summary.histogram('histogram loss', self.loss)\n",
    "            self.summary_op = tf.summary.merge_all()\n",
    "\n",
    "    def build(self):\n",
    "        \"\"\"\n",
    "        Builds the computation graph\n",
    "        \"\"\"\n",
    "        self.x_ph = tf.placeholder(tf.float32, [None, \n",
    "                                                None,\n",
    "                                                self.signalDim,\n",
    "                                                1]) \n",
    "        self.y_ph = tf.placeholder(tf.float32, [None,\n",
    "                                                self.signalDim,\n",
    "                                                self.signalDim])\n",
    "        self.data_generator()\n",
    "        self.data_generator(trainingBatch=False)\n",
    "        self.inference()\n",
    "        self.loss()\n",
    "        self.optimize()\n",
    "        self.additionalEvalMetrics()\n",
    "        self.summary()\n",
    "    \n",
    "    def train_one_epoch(self, sess, saver, writer, epoch, step):\n",
    "#        start_time = time.time()\n",
    "        self.isTraining = True\n",
    "        _, l, summaries = sess.run([self.opt, \n",
    "                                    self.loss,\n",
    "                                    self.summary_op],\n",
    "                                   feed_dict={self.x_ph: self.train_x_new,\n",
    "                                              self.y_ph: self.train_y_new})\n",
    "        writer.add_summary(summaries, global_step=step)\n",
    "        #if (step + 1) % self.skipStep == 0:\n",
    "        #    print('training Loss at step {0}: {1}'.format(step, l))\n",
    "        step += 1\n",
    "        saver.save(sess, 'checkpoints/cryoem_'+self.vers+'/cpoint', global_step=step)\n",
    "#        print('Average loss at epoch {0}: {1}'.format(epoch, l))\n",
    "#        print('Took: {0} seconds'.format(time.time() - start_time))\n",
    "        return step\n",
    "\n",
    "    def train(self, n_epochs):\n",
    "        \"\"\"\n",
    "        Calls the training ops and prepares the training data\n",
    "        for the next batch in a parallel process.\n",
    "        \"\"\"\n",
    "        safe_mkdir('checkpoints')\n",
    "        safe_mkdir('checkpoints/cryoem_'+self.vers)\n",
    "        writer = tf.summary.FileWriter('./graphs/cryoem_'+self.vers, tf.get_default_graph())\n",
    "\n",
    "        tVars = tf.trainable_variables()\n",
    "        defGraph = tf.get_default_graph()\n",
    "\n",
    "        for v in defGraph.get_collection(tf.GraphKeys.GLOBAL_VARIABLES): \n",
    "            if (('bnorm_' in v.name) and\n",
    "                ('/Adam' not in v.name) and\n",
    "                ('Adagrad' not in v.name) and\n",
    "                (v not in tVars )):\n",
    "                tVars.append(v)\n",
    "                \n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            saver = tf.train.Saver(var_list= tVars)\n",
    "            ckpt = tf.train.get_checkpoint_state(os.path.dirname('checkpoints/cryoem_'+self.vers+'/cpoint'))\n",
    "\n",
    "            if ckpt and ckpt.model_checkpoint_path:\n",
    "                saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "            \n",
    "            step = self.gstep.eval()\n",
    "\n",
    "            for epoch in range(n_epochs):\n",
    "                self.train_x = np.copy(self.train_x_new)\n",
    "                self.train_y = np.copy(self.train_y_new)\n",
    "                \n",
    "                pool = ThreadPool(processes=1)\n",
    "                async_result = pool.apply_async(self.data_generator,())\n",
    "                step = self.train_one_epoch(sess, saver, writer, epoch, step)\n",
    "                pool.close()\n",
    "                pool.join()\n",
    "\n",
    "                if ( ((step + 1) % self.skipStep == 0) and (step>self.evalAfterStep ) ) :\n",
    "                    self.testEval(sess, writer, epoch, step,self.evalNTimes)\n",
    "                    \n",
    "                    \n",
    "        writer.close()\n",
    "        self.isTraining = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = objGenNetwork(\n",
    "        signalDim = 5,\n",
    "         nObservationsPerSignal = 128,\n",
    "         noise = 2,\n",
    "         minibatchSize = 32,\n",
    "         testSampleSize = 64,\n",
    "         evalNTimes=16,\n",
    "         lr = 0.0001,\n",
    "         training = True,\n",
    "         skipStep = 10,\n",
    "         nProcessesDataPrep=2,\n",
    "         vers='deneme',\n",
    "         evalAfterStep=0\n",
    "         )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "None values not supported.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-2f52417d930d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-85ad4de6a3c4>\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    387\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    390\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madditionalEvalMetrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-85ad4de6a3c4>\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    313\u001b[0m             \u001b[0mgrads_and_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dropgrad'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m                 \u001b[0mgrads_and_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-85ad4de6a3c4>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    313\u001b[0m             \u001b[0mgrads_and_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dropgrad'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m                 \u001b[0mgrads_and_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36mdropout\u001b[0;34m(x, keep_prob, noise_shape, seed, name)\u001b[0m\n\u001b[1;32m   2306\u001b[0m   \"\"\"\n\u001b[1;32m   2307\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dropout\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2308\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"x\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2309\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2310\u001b[0m       raise ValueError(\"x has to be a floating point tensor since it's going to\"\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, preferred_dtype)\u001b[0m\n\u001b[1;32m    996\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    997\u001b[0m       \u001b[0mpreferred_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpreferred_dtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 998\u001b[0;31m       as_ref=False)\n\u001b[0m\u001b[1;32m    999\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1000\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36minternal_convert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, ctx)\u001b[0m\n\u001b[1;32m   1092\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1093\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1094\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1095\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1096\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    215\u001b[0m                                          as_ref=False):\n\u001b[1;32m    216\u001b[0m   \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name, verify_shape)\u001b[0m\n\u001b[1;32m    194\u001b[0m   tensor_value.tensor.CopyFrom(\n\u001b[1;32m    195\u001b[0m       tensor_util.make_tensor_proto(\n\u001b[0;32m--> 196\u001b[0;31m           value, dtype=dtype, shape=shape, verify_shape=verify_shape))\n\u001b[0m\u001b[1;32m    197\u001b[0m   \u001b[0mdtype_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr_value_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAttrValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m   const_tensor = g.create_op(\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/framework/tensor_util.py\u001b[0m in \u001b[0;36mmake_tensor_proto\u001b[0;34m(values, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    422\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"None values not supported.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    425\u001b[0m     \u001b[0;31m# if dtype is provided, forces numpy array to be the type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m     \u001b[0;31m# provided if possible.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: None values not supported."
     ]
    }
   ],
   "source": [
    "model.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
