{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still work in progress. Will be constructing a graph neural network to input the data and find the L2 orbit loss. Afterward I will also try a GAN to do the same task and compare their performances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython import display\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import utils\n",
    "from skimage.util.shape import view_as_windows as viewW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strided_indexing_roll(a, r):\n",
    "    # https://stackoverflow.com/questions/20360675/roll-rows-of-a-matrix-independently\n",
    "    # Concatenate with sliced to cover all rolls\n",
    "    a_ext = np.concatenate((a,a[:,:-1]),axis=1)\n",
    "\n",
    "    # Get sliding windows; use advanced-indexing to select appropriate ones\n",
    "    n = a.shape[1]\n",
    "    return viewW(a_ext,(1,n))[np.arange(len(r)), (n-r)%n,0]\n",
    "\n",
    "def dense(x, scope, num_h):\n",
    "    \"\"\"\n",
    "    standard affine layer\n",
    "    scope = name tf variable scope\n",
    "    num_h = number of hidden units\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(scope):\n",
    "        n_x = x.get_shape().as_list()[-1]\n",
    "        w = tf.get_variable('w', [n_x, num_h], initializer=tf.random_normal_initializer(stddev=0.04))\n",
    "        b = tf.get_variable('b', [num_h], initializer=tf.constant_initializer(0))\n",
    "        return tf.matmul(x, w)+b\n",
    "\n",
    "def noise(X, std=1):\n",
    "    \"\"\"\n",
    "    adds gaussian noise with unit variance\n",
    "    \"\"\"\n",
    "    noise = tf.random_normal(shape=tf.shape(X), mean=0.0, stddev=std, dtype=tf.float32) \n",
    "    return(input_layer + noise)\n",
    "    \n",
    "def conv_1Ds(x, scope, filter_width, n_kernel, stride=1):\n",
    "    \"\"\"\n",
    "    1d convolutions over multiple 1d images\n",
    "    scope        = name tf variable scope\n",
    "    filter_width = receptive field of kernel\n",
    "    n_kernel     = # of kernels\n",
    "    stride       = locations\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(scope):\n",
    "        n_x = x.get_shape().as_list()[-1]\n",
    "        w = tf.get_variable('w',\n",
    "                            [1, filter_width, n_x, n_kernel],\n",
    "                            initializer=tf.random_normal_initializer(stddev=0.04))\n",
    "        b = tf.get_variable('b', [n_kernel], initializer=tf.constant_initializer(0))\n",
    "        return tf.nn.convolution(x, w, padding='SAME', strides=[1, 1, stride,1])+b    \n",
    "    \n",
    "    \n",
    "def convOneDImages(X, reuse=False):\n",
    "    \"\"\"\n",
    "    Makes convolutions over 1D images and takes their averages\n",
    "    for given object. Currently trying an approach similar to GNN.\n",
    "    \"\"\"\n",
    "    with tf.variable_scope('convOneDImages', reuse=reuse):\n",
    "        h = tf.nn.leaky_relu(conv_1Ds(X, 'c0', r_f=5, n_f=16, stride=2), 0.1) #32x16\n",
    "        h = tf.nn.leaky_relu(conv_1Ds(h, 'c1', r_f=5, n_f=32, stride=2), 0.1) #16x32\n",
    "        h = tf.nn.leaky_relu(conv_1Ds(h, 'c2', r_f=5, n_f=64, stride=2), 0.1) #8x64\n",
    "        h = tf.nn.leaky_relu(conv_1Ds(h, 'c3', r_f=5, n_f=128, stride=2), 0.1)#4x128\n",
    "        h = tf.reshape(h, [n_batch* model_n_img_per_obj, -1])\n",
    "        \n",
    "        h = tf.nn.leaky_relu(dense(h, 'fc_0', 128*4), 0.1)\n",
    "        h = tf.nn.leaky_relu(dense(h, 'fc_1', 128*4), 0.1)\n",
    "        h = tf.reshape(h, [n_batch, model_n_img_per_obj, -1]) \n",
    "\n",
    "        h = tf.reduce_mean(h, 1)\n",
    "        return h\n",
    "\n",
    "\n",
    "\n",
    "class objGenNetwork(object):\n",
    "    \"\"\"\n",
    "    not finished.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 img_dim = 64,\n",
    "                 file_n_img_per_obj = 200,\n",
    "                 model_n_img_per_obj = 32,\n",
    "                 noise = 0.5,\n",
    "                 shift_n = 9\n",
    "                 minibatch_size = 50,\n",
    "                 test_samples_per_batch = 10,\n",
    "                 lr = 0.001):\n",
    "        \n",
    "        self.img_dim = img_dim\n",
    "        self.file_n_img_per_obj = file_n_img_per_obj\n",
    "        self.model_n_img_per_obj = model_n_img_per_obj\n",
    "        self.noise = noise\n",
    "        self.shift_n = shift_n\n",
    "        self.minibatch_size = minibatch_size\n",
    "        self.test_samples_per_batch = test_samples_per_batch\n",
    "        self.lr = lr\n",
    "        \n",
    "        self.store = pd.HDFStore('generated_data.h5')\n",
    "        self.nrows = store.get_storer('gen_2d_objects').nrows\n",
    "        self.chunksize = minibatch_size + test_samples_per_batch\n",
    "        \n",
    "        self.gstep = tf.Variable(0, \n",
    "                                 dtype=tf.int32, \n",
    "                                 trainable=False,\n",
    "                                 name='global_step')\n",
    "\n",
    "        \n",
    "    def inference(X):\n",
    "        h = noise(X,self.noise)\n",
    "        h = convOneDImages(X)\n",
    "        h = tf.nn.leaky_relu(dense(h, 'fc_0', 1024), 0.1)\n",
    "        h = tf.nn.leaky_relu(dense(h, 'fc_1', 2048), 0.1)\n",
    "        self.logits = dense(h, 'p_logits', img_dim*img_dim)\n",
    "        \n",
    "    def data_reader(store_obj=self.store,\n",
    "                    nrows = self.nrows,\n",
    "                    chunksize = self.chunksize,\n",
    "                    minibatch_size = self.minibatch_size,\n",
    "                    test_samples_per_batch = self.test_samples_per_batch,\n",
    "                    img_dim = self.img_dim,\n",
    "                    noise = self.noise,\n",
    "                    shift_n = self.shift_n\n",
    "                    file_n_img_per_obj=self.file_n_img_per_obj,\n",
    "                    model_n_img_per_obj=self.model_n_img_per_obj,\n",
    "                    i=self.data_iter):\n",
    "        '''\n",
    "        Reads chunks of data starting for the i'th iteration.\n",
    "        Prepares train_x, test_x, train_y, test_y \n",
    "        '''\n",
    "        #reads the 2d objects and separates them for training and test sets.\n",
    "        chunk_2d = store_obj.select('gen_2d_objects',\n",
    "                                    start=i*chunksize*img_dim,\n",
    "                                    stop=(i+1)*chunksize*img_dim)\n",
    "        train_y=chunk_2d.iloc[:minibatch_size*img_dim,:img_dim].values.reshape([minibatch_size,\\\n",
    "                                                                                img_dim,img_dim]).astype('float32')\n",
    "        test_y=chunk_2d.iloc[minibatch_size*img_dim:,:img_dim].values.reshape([test_samples_per_batch,\\\n",
    "                                                                               img_dim,img_dim]).astype('float32')\n",
    "        #reads the 1d images, formats them and separates them for training and test sets.\n",
    "        chunk_1d = store_obj.select('gen_1d_data',\n",
    "                                    start=i*chunksize*file_n_img_per_obj,\n",
    "                                    stop=(i+1)*chunksize*file_n_img_per_obj)\n",
    "        # random image translations\n",
    "        if(shift_n > 0):\n",
    "            temp_1d_np = np.zeros((chunk_1d.shape[0],chunk_1d.shape[1]+shift_n*2))\n",
    "            col_start = np.random.randint(-shift_n, shift_n, (chunk_1d.shape[0],))\n",
    "            temp_1d_np[:,shift_n:img_dim+shift_n] = chunk_1d\n",
    "            chunk_1d = strided_indexing_roll(temp_1d_np, col_start)[:,shift_n:img_dim+shift_n]\n",
    "\n",
    "        # add gaussian noise\n",
    "        if(noise > 0):\n",
    "            chunk_1d = chunk_1d + np.random.normal(0,1,chunk_1d.shape)\n",
    "\n",
    "            \n",
    "        train_x = chunk_1d.iloc[:minibatch_size*file_n_img_per_obj,:img_dim].values.reshape([minibatch_size,\\\n",
    "                                                                                             file_n_img_per_obj,\\\n",
    "        \n",
    "                                                                                             img_dim])\n",
    "        test_x = chunk_1d.iloc[minibatch_size*file_n_img_per_obj:,:img_dim].values.reshape([test_samples_per_batch,\\\n",
    "                                                                                            file_n_img_per_obj,\\\n",
    "                                                                                            img_dim])\n",
    "        i += 1\n",
    "        #return(train_x,test_x,train_y,test_y)\n",
    "        self.train_x = train_x\n",
    "        self.test_x = test_x\n",
    "        self.train_y = train_y\n",
    "        self.test_y = test_y\n",
    "        self.train_labels = train_y.reshape((minibatch_size, img_dim*img_dim))\n",
    "        self.test_labels = test_y.reshape((minibatch_size, img_dim*img_dim))\n",
    "\n",
    "    def loss(self):\n",
    "        '''\n",
    "        define loss function\n",
    "        use softmax cross entropy with logits as the loss function\n",
    "        compute mean cross entropy, softmax is applied internally\n",
    "        '''\n",
    "        # \n",
    "        with tf.name_scope('loss'):\n",
    "            entropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=self.train_labels, logits=self.logits)\n",
    "            self.loss = tf.reduce_mean(entropy, name='loss')\n",
    "\n",
    "\n",
    "def eval(self):\n",
    "        \"\"\"\n",
    "        Calculates means of correct predictions per image\n",
    "        in a given batch.\n",
    "        \"\"\"\n",
    "        with tf.name_scope('predict'):           \n",
    "            preds = tf.nn.sigmoid(self.logits)\n",
    "            correct_preds = tf.equal(tf.round(preds), Y)\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_preds, tf.float32))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
