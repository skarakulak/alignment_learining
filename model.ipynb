{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from skimage.transform import rotate, rescale\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.path import Path\n",
    "import fiona\n",
    "import shapely.geometry as geometry\n",
    "from shapely.geometry import Point\n",
    "from shapely.affinity import rotate\n",
    "from descartes import PolygonPatch\n",
    "import pylab as pl\n",
    "import multiprocessing as mp\n",
    "import pathlib\n",
    "import datetime\n",
    "from src import *\n",
    "\n",
    "np.random.seed(99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some examples of the random two-dimentional objects that we are going to use. They are generated by concave hulls of random points in the two-dimentional space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_point = 60; alpha = .2\n",
    "fig = pl.figure(figsize=(12,12))\n",
    "for i in range(64):\n",
    "    a = np.random.uniform(16,48,size=(n_point,2))\n",
    "    points = [Point(a[i]) for i in range(n_point)]\n",
    "    concave_hull, edge_points = alpha_shape(points, alpha=alpha)\n",
    "\n",
    "    ax = fig.add_subplot(8,8,i+1)\n",
    "    ax.set_yticklabels([]); \n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_xlim([0, 64]) #ax.set_xlim([x_min-margin, x_max+margin])\n",
    "    ax.set_ylim([0, 64])\n",
    "    patch = PolygonPatch(concave_hull.buffer(1), fc='#999999', ec='#000000', fill=True, zorder=-1)\n",
    "    ax.add_patch(patch)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import multiprocessing as mp\n",
    "from multiprocessing.pool import ThreadPool\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from src import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense(x, scope, num_h, n_x):\n",
    "    \"\"\"\n",
    "    standard affine layer\n",
    "    scope = name tf variable scope\n",
    "    num_h = number of hidden units\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(scope):\n",
    "        w = tf.get_variable('w', [n_x, num_h], initializer=tf.random_normal_initializer(stddev=0.04))\n",
    "        b = tf.get_variable('b', [num_h], initializer=tf.constant_initializer(0))\n",
    "        return tf.matmul(x, w)+b\n",
    "\n",
    "def conv(x, scope, filter_h,filter_w, n_kernel, stride_h=1,stride_w=1, padding='SAME'):\n",
    "    \"\"\"\n",
    "    Convolutional layer\n",
    "    scope        = name tf variable scope\n",
    "    filter_width = receptive field of kernel\n",
    "    n_kernel     = # of kernels\n",
    "    stride       = locations\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(scope, reuse=tf.AUTO_REUSE):\n",
    "        n_x = x.get_shape().as_list()[-1]\n",
    "        w = tf.get_variable('w',\n",
    "                            [filter_h, filter_w, n_x, n_kernel],\n",
    "                            initializer=tf.random_normal_initializer(stddev=0.04))\n",
    "        b = tf.get_variable('b', [n_kernel], initializer=tf.constant_initializer(0))\n",
    "        return tf.nn.convolution(x, w, padding=padding, strides=[stride_h, stride_w])+b    \n",
    "\n",
    "\n",
    "def maxpool(X, scope, filter_h,filter_w, stride_h,stride_w, padding='VALID'):\n",
    "    \"\"\"\n",
    "    Maxpool operation\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(scope, reuse=tf.AUTO_REUSE) as scope:\n",
    "        pool = tf.nn.max_pool(X, \n",
    "                            ksize=[1, filter_h, filter_w, 1], \n",
    "                            strides=[1, stride_h, stride_w, 1],\n",
    "                            padding=padding)\n",
    "    return pool    \n",
    "    \n",
    "def upsample(X, ratio=2):\n",
    "    \"\"\"\n",
    "    Takes a 4D image tensor and increases spatial resolution by replicating values\n",
    "    \"\"\"\n",
    "    n_h, n_w = X.get_shape().as_list()[1:3]\n",
    "    return tf.image.resize_nearest_neighbor(X, [n_h*ratio, n_w*ratio])\n",
    "    \n",
    "    \n",
    "def conv_1D_Images(X,batch_size,n_images, reuse=False):\n",
    "    \"\"\"\n",
    "    Makes convolutions over 1D images  in a tensor of size \n",
    "    (batch_size,n_images, imgsize,1 ). Returns the average \n",
    "    values of the encodings of 1D images. \n",
    "    \"\"\"\n",
    "\n",
    "    with tf.variable_scope('convOneDImages', reuse=reuse):\n",
    "        h = tf.nn.leaky_relu(conv(X, 'conv0',1,5,32,1,2), 0.1)    #32x32\n",
    "        h = tf.nn.leaky_relu(conv(h, 'conv1', 1,3,64,1,1), 0.1)   #32x64\n",
    "        h = maxpool(h,'pool1', 1,2,1,2,'VALID')                        #16x64\n",
    "        h = tf.nn.leaky_relu(conv(h, 'conv2', 1,3,128,1,1), 0.1)  #16x128\n",
    "        h = maxpool(h,'pool2', 1,2,1,2,'VALID')                        #8x128\n",
    "        h = tf.nn.leaky_relu(conv(h, 'conv3', 1,3,256,1,1), 0.1)  #8x256\n",
    "        h = maxpool(h,'pool3', 1,2,1,2,'VALID')                        #4x256\n",
    "        h = tf.reshape(h, [batch_size*n_images, -1])\n",
    "        \n",
    "        h = tf.nn.leaky_relu(dense(h, 'fc_0', 512,4*256), 0.1)\n",
    "        h = tf.nn.leaky_relu(dense(h, 'fc_1', 512,512), 0.1)\n",
    "        h = tf.reshape(h, [batch_size, n_images, 512]) \n",
    "\n",
    "        h = tf.reduce_mean(h, 1)\n",
    "        return h\n",
    "\n",
    "\n",
    "def gen_2D_Obj(X, batch_size, enc_dim = 512, reuse=False):\n",
    "    \"\"\"\n",
    "    Takes encoding produced by the 1D images as input and\n",
    "    generates a 2D image of the object\n",
    "    \"\"\"\n",
    "    with tf.variable_scope('2D_object_generator', reuse=reuse):\n",
    "        h = tf.nn.leaky_relu(dense(X, 'hz', num_h=6*6*32,n_x=enc_dim), 0.1)\n",
    "        h = tf.nn.leaky_relu(dense(h, 'hz1', num_h=6*6*128,n_x=6*6*32), 0.1)\n",
    "        h = tf.reshape(h, [batch_size, 6, 6, 128]) \n",
    "        h = upsample(h)                                          #12x12x128\n",
    "        h = tf.nn.leaky_relu(conv(h, 'h1', 3,3, n_kernel=64), 0.1)    #12x12x64\n",
    "        h = upsample(h)                                          #24x24x64\n",
    "        h = tf.nn.leaky_relu(conv(h, 'h2', 3,3, n_kernel=32), 0.1)    #24x24x32\n",
    "        h = upsample(h)                                          #48x48x32\n",
    "        h = tf.nn.leaky_relu(conv(h, 'h3', 3,3, n_kernel=16), 0.1)    #48x48x16\n",
    "        h = conv(h, 'hx', 1,1,1)\n",
    "        return h    \n",
    "    \n",
    "\n",
    "class objGenNetwork(object):\n",
    "    \"\"\"\n",
    "    Implementation of the model\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 imgDim = 64,\n",
    "                 o_imgDim = 48,\n",
    "                 model_n_img_per_obj = 32,\n",
    "                 noise = 0.5,\n",
    "                 minibatchSize = 50,\n",
    "                 testSampleSize = 1000,\n",
    "                 lr = 0.001,\n",
    "                 nProcess_dataprep = 4,\n",
    "                 training = True,\n",
    "                 shift_n = 8,\n",
    "                 vers='v1'\n",
    "                ):\n",
    "        \n",
    "        self.imgDim = imgDim\n",
    "        self.o_imgDim = o_imgDim\n",
    "        self.model_n_img_per_obj = model_n_img_per_obj\n",
    "        self.noise = noise\n",
    "        self.minibatchSize = minibatchSize\n",
    "        self.testSampleSize = testSampleSize\n",
    "        self.lr = lr\n",
    "        self.nProcess_dataprep = nProcess_dataprep\n",
    "        self.training = training\n",
    "        self.shift_n = shift_n\n",
    "        self.skip_step = 1\n",
    "        self.vers = vers\n",
    "        \n",
    "        self.gstep = tf.Variable(0, \n",
    "                                 dtype=tf.int32, \n",
    "                                 trainable=False,\n",
    "                                 name='global_step')\n",
    "        self.train_x = None\n",
    "        self.train_y = None\n",
    "        self.test_x = None\n",
    "        self.test_y = None\n",
    "\n",
    "    def data_generator(self):\n",
    "\n",
    "        \"\"\"\n",
    "        Generates batches of random objects and their 1D \n",
    "        density images from random rotations. Objects are \n",
    "        encoded in a NxN occupancy map, where the M[i,j] is\n",
    "        1 if the object occupies the area between the coordinates\n",
    "        (i,j),(i+1,j),(i,j+1),(i+1,j+1), and 0 otherwise. Gaussian\n",
    "        noise with N(0,(self.noise)^2 ) is applied over 1D images.\n",
    "        Additionally we shift 1D images randomly to left and right \n",
    "        by 8 pixels\n",
    "        \"\"\"\n",
    "        nProcess_dataprep=self.nProcess_dataprep\n",
    "        minibatchSize = self.minibatchSize\n",
    "        testSampleSize = self.testSampleSize\n",
    "        imgDim = self.imgDim\n",
    "        o_imgDim = self.o_imgDim\n",
    "        noise = self.noise\n",
    "        model_n_img_per_obj=self.model_n_img_per_obj\n",
    "        trainingBatch=True\n",
    "        #shift_n = self.shift_n \n",
    "        #translations are done within the gen_rand_poly_images function\n",
    "        \n",
    "        batches = minibatchSize if trainingBatch else testSampleSize\n",
    "        \n",
    "        pool = mp.Pool(processes=nProcess_dataprep)\n",
    "        results = pool.map(gen_rand_poly_images,[model_n_img_per_obj] * batches )\n",
    "        pool.close(); pool.join()\n",
    "        \n",
    "        batch_x = np.expand_dims(np.array([k[1] for k in results],\n",
    "                                          dtype='float32'),\n",
    "                                 axis=3)\n",
    "        batch_y = np.expand_dims(np.array([k[2] for k in results],\n",
    "                                          dtype='float32'),\n",
    "                                 axis=4)\n",
    "        \n",
    "        # add gaussian noise\n",
    "        if(noise > 0):\n",
    "            batch_x = batch_x + np.random.normal(0,noise,batch_x.shape)\n",
    "\n",
    "        if trainingBatch:\n",
    "            self.train_x_new = batch_x\n",
    "            self.train_y_new = batch_y\n",
    "        else:\n",
    "            self.new_test_x = batch_x\n",
    "            self.new_test_y = batch_y\n",
    "        \n",
    "        \n",
    "    def inference(self):\n",
    "        h = conv_1D_Images(self.x_ph, self.minibatchSize, self.model_n_img_per_obj)\n",
    "        self.logits = gen_2D_Obj(h, self.minibatchSize)\n",
    "\n",
    "        \n",
    "    def loss(self):\n",
    "        \"\"\"\n",
    "        Defines loss function\n",
    "        We use cross entropy with logits over the pixels. Computes the minimum\n",
    "        loss over all rotations\n",
    "        \"\"\"\n",
    "        # \n",
    "        with tf.name_scope('loss'):\n",
    "            tiled_logits = tf.tile(tf.expand_dims(self.logits, 1),[1,360,1,1,1])\n",
    "            entropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=self.y_ph, logits=tiled_logits)\n",
    "            entropy = tf.reduce_sum(entropy, axis = [2,3,4])\n",
    "            entropy = tf.reduce_min(entropy,axis = 1)\n",
    "            self.loss = tf.reduce_mean(entropy, name='loss')\n",
    "\n",
    "    def optimize(self):\n",
    "        \"\"\"\n",
    "        Optimization op\n",
    "        \"\"\"\n",
    "        self.opt = tf.train.AdadeltaOptimizer(self.lr).minimize(self.loss, \n",
    "                                                global_step=self.gstep)\n",
    "\n",
    "    def eval_graph(self):\n",
    "        \"\"\"\n",
    "        Takes the most accurate rotation of the true object\n",
    "        as reference and calculates the average accuracy of\n",
    "        the occupancy grid of the object\n",
    "        \"\"\"\n",
    "        with tf.name_scope('predict'):           \n",
    "            self.predictions = tf.nn.sigmoid(self.logits)\n",
    "        with tf.name_scope('prediction_eval'):\n",
    "            tiledPreds = tf.tile(tf.expand_dims(self.predictions, 1),\n",
    "                                 [1,360,1,1, 1])\n",
    "            correctPreds = tf.equal(tf.round(tiledPreds),\n",
    "                                    self.y_ph)\n",
    "            hitsOnRotations = tf.reduce_sum(tf.cast(correctPreds,tf.float32),\n",
    "                                            axis = [2,3,4])\n",
    "            mostHits = tf.reduce_max(hitsOnRotations,axis=1)\n",
    "            self.accuracy = tf.divide(tf.reduce_mean(mostHits),64.*64.)\n",
    "        \n",
    "        \n",
    "    def eval_once(self, sess, writer, epoch, step):\n",
    "        accuracy_batch, summaries = sess.run([self.accuracy,\n",
    "                                              self.summary_op],\n",
    "                                             feed_dict={self.x_ph: self.train_x_new,\n",
    "                                                        self.y_ph: self.train_y_new})\n",
    "        writer.add_summary(summaries, global_step=step)\n",
    "        print('Accuracy at step {0}: {1} '.format(step,accuracy_batch))\n",
    "        \n",
    "\n",
    "\n",
    "    def summary(self):\n",
    "        \"\"\"\n",
    "        Summary for TensorBoard\n",
    "        \"\"\"\n",
    "        with tf.name_scope('summaries'):\n",
    "            tf.summary.scalar('loss', self.loss)\n",
    "            tf.summary.scalar('accuracy', self.accuracy)\n",
    "            tf.summary.histogram('histogram loss', self.loss)\n",
    "            self.summary_op = tf.summary.merge_all()\n",
    "\n",
    "    def build(self):\n",
    "        \"\"\"\n",
    "        Builds the computation graph\n",
    "        \"\"\"\n",
    "        self.x_ph = tf.placeholder(tf.float32, [self.minibatchSize, \n",
    "                                                self.model_n_img_per_obj,\n",
    "                                                self.imgDim,\n",
    "                                                1]) \n",
    "        self.y_ph = tf.placeholder(tf.float32, [self.minibatchSize,\n",
    "                                                360,\n",
    "                                                self.o_imgDim,\n",
    "                                                self.o_imgDim,\n",
    "                                                1])\n",
    "        self.data_generator()\n",
    "        self.inference()\n",
    "        self.loss()\n",
    "        self.optimize()\n",
    "        self.eval_graph()\n",
    "        self.summary()\n",
    "    \n",
    "    def train_one_epoch(self, sess, saver, writer, epoch, step):\n",
    "#        start_time = time.time()\n",
    "        self.training = True\n",
    "        _, l, summaries = sess.run([self.opt, \n",
    "                                    self.loss,\n",
    "                                    self.summary_op],\n",
    "                                   feed_dict={self.x_ph: self.train_x_new,\n",
    "                                              self.y_ph: self.train_y_new})\n",
    "        writer.add_summary(summaries, global_step=step)\n",
    "        if (step + 1) % self.skip_step == 0:\n",
    "            print('Loss at step {0}: {1}'.format(step, l))\n",
    "        step += 1\n",
    "        saver.save(sess, 'checkpoints/cryoem/cryoem_'+self.vers, step)\n",
    "#        print('Average loss at epoch {0}: {1}'.format(epoch, l))\n",
    "#        print('Took: {0} seconds'.format(time.time() - start_time))\n",
    "        return step\n",
    "\n",
    "    def train(self, n_epochs):\n",
    "        \"\"\"\n",
    "        We prepare the training data in a paralel process during training.\n",
    "        When the next batch is ready, it is first used as a test data, and\n",
    "        we report the prediction accuracy over that batch. Afterwards this\n",
    "        new batch is used for training\n",
    "        \"\"\"\n",
    "        safe_mkdir('checkpoints')\n",
    "        safe_mkdir('checkpoints/cryoem_adadelta')\n",
    "        writer = tf.summary.FileWriter('./graphs/cryoem_'+self.vers, tf.get_default_graph())\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            saver = tf.train.Saver()\n",
    "            ckpt = tf.train.get_checkpoint_state(os.path.dirname('checkpoints/cryoem_adadelta/cryoem_'+self.vers))\n",
    "            if ckpt and ckpt.model_checkpoint_path:\n",
    "                saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "            \n",
    "            step = self.gstep.eval()\n",
    "\n",
    "            for epoch in range(n_epochs):\n",
    "                self.train_x = self.train_x_new\n",
    "                self.train_y = self.train_y_new\n",
    "\n",
    "                pool = ThreadPool(processes=1)\n",
    "                async_result = pool.apply_async(self.data_generator,())\n",
    "                step = self.train_one_epoch(sess, saver, writer, epoch, step)\n",
    "                pool.close()\n",
    "                pool.join()\n",
    "                self.eval_once(sess, writer, epoch, step)\n",
    "                \n",
    "                y_pred = sess.run(self.predictions,feed_dict={self.x_ph: self.train_x_new})\n",
    "                plt.imsave('0true.png',self.train_y_new[5][0][:,:,0])\n",
    "                plt.imsave('0pred.png',y_pred[5,:,:,0]>0.5)\n",
    "                plt.imsave('1true.png',self.train_y_new[6][0][:,:,0])\n",
    "                plt.imsave('1pred.png',y_pred[6,:,:,0]>0.5)\n",
    "                \n",
    "        writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name histogram loss is illegal; using histogram_loss instead.\n"
     ]
    }
   ],
   "source": [
    "# Note that the data generation process would utilize 4 CPU\n",
    "# threads. If the model would train on GPU, you may set the\n",
    "# 'nProcess_dataprep' argument equal to 'mp.cpu_count()'\n",
    "model = objGenNetwork(noise=0,model_n_img_per_obj=64, lr=1,nProcess_dataprep=4, vers='sigma0')\n",
    "model.build()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 0: 1596.9141845703125\n",
      "Accuracy at step 1: 0.49705079197883606 \n",
      "Loss at step 1: 1594.97998046875\n",
      "Accuracy at step 2: 0.5083593726158142 \n",
      "Loss at step 2: 1585.26708984375\n",
      "Accuracy at step 3: 0.5082421898841858 \n",
      "Loss at step 3: 1526.8699951171875\n",
      "Accuracy at step 4: 0.512158215045929 \n",
      "Loss at step 4: 1113.150146484375\n",
      "Accuracy at step 5: 0.5102783441543579 \n",
      "Loss at step 5: 824.8446044921875\n",
      "Accuracy at step 6: 0.38819336891174316 \n",
      "Loss at step 6: 1110.2320556640625\n",
      "Accuracy at step 7: 0.4976855516433716 \n",
      "Loss at step 7: 665.203369140625\n",
      "Accuracy at step 8: 0.44819337129592896 \n",
      "Loss at step 8: 1426.599365234375\n",
      "Accuracy at step 9: 0.5086279511451721 \n",
      "Loss at step 9: 1215.6878662109375\n",
      "Accuracy at step 10: 0.5098535418510437 \n",
      "Loss at step 10: 563.6553344726562\n",
      "Accuracy at step 11: 0.516894519329071 \n",
      "Loss at step 11: 470.4573059082031\n",
      "Accuracy at step 12: 0.5219433307647705 \n",
      "Loss at step 12: 462.79522705078125\n",
      "Accuracy at step 13: 0.5221777558326721 \n",
      "Loss at step 13: 403.3318786621094\n",
      "Accuracy at step 14: 0.5139746069908142 \n",
      "Loss at step 14: 463.5138244628906\n",
      "Accuracy at step 15: 0.5189013481140137 \n",
      "Loss at step 15: 431.5117492675781\n",
      "Accuracy at step 16: 0.5202734470367432 \n",
      "Loss at step 16: 416.1614990234375\n",
      "Accuracy at step 17: 0.5158349871635437 \n",
      "Loss at step 17: 447.4316101074219\n",
      "Accuracy at step 18: 0.5220507979393005 \n",
      "Loss at step 18: 403.7596130371094\n",
      "Accuracy at step 19: 0.5208789110183716 \n",
      "Loss at step 19: 416.7011413574219\n",
      "Accuracy at step 20: 0.5246630907058716 \n",
      "Loss at step 20: 377.18792724609375\n",
      "Accuracy at step 21: 0.5234375 \n",
      "Loss at step 21: 389.5488586425781\n",
      "Accuracy at step 22: 0.5185253620147705 \n",
      "Loss at step 22: 416.5358581542969\n",
      "Accuracy at step 23: 0.522167980670929 \n",
      "Loss at step 23: 398.5601501464844\n",
      "Accuracy at step 24: 0.5227587819099426 \n",
      "Loss at step 24: 378.6490478515625\n",
      "Accuracy at step 25: 0.5181249976158142 \n",
      "Loss at step 25: 429.72625732421875\n",
      "Accuracy at step 26: 0.5221142768859863 \n",
      "Loss at step 26: 409.8392639160156\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "model.train(n_epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver()\n",
    "    ckpt = tf.train.get_checkpoint_state(os.path.dirname('checkpoints/cryoem/cryoem_sigma1'))\n",
    "    if ckpt and ckpt.model_checkpoint_path:\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "\n",
    "    y_pred = sess.run(model.predictions,feed_dict={model.x_ph: model.train_x_new})\n",
    "\n",
    "\n",
    "\n",
    "fig=plt.figure(figsize=(20, 8))\n",
    "columns = 2\n",
    "rows = 5\n",
    "for i in range(0, 5):\n",
    "    fig.add_subplot(5, 2, i*2+1)\n",
    "    plt.imshow(model.train_y_new[0][0][:,:,0],interpolation='nearest', cmap=\"gray\")\n",
    "    fig.add_subplot(5, 2, i*2+2)\n",
    "    plt.imshow(y_pred[i,:,:,0]>0.5,interpolation='nearest', cmap=\"gray\")\n",
    "    plt.imshow(img)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
